{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from scipy import ndimage\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "root='/home/cc/Data/stanford_car/'\n",
    "pickle_file = 'train_dataset_1000_1500.pickle'\n",
    "train_dataset = pickle.load( open( pickle_file, \"rb\" ) )\n",
    "print(type(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_dataset['data']\n",
    "labels = train_dataset['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 227\n",
    "image_channels = 3\n",
    "num_classes = 196\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None, image_size, image_size, image_channels])\n",
    "y = tf.placeholder(tf.float32,[None, num_classes])\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, strides, padding):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=padding)\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "def Normalization(x):\n",
    "    Norm=tf.nn.local_response_normalization(x, depth_radius=5, bias=1.0, alpha=0.0001, beta=0.75, name=None)\n",
    "    return Norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer1_conv = {'in_size': (227,227), 'dept': 3, 'out_size': (55,55), 'fsize': (11, 11), 'nof':96, 'pad': 'VALID', 'strid': 4}\n",
    "layer1_maxpool = {'in_size': (55,55), 'dept': 96, 'out_size': (27,27), 'fsize': (3, 3), 'strid': 2, 'pad': 'VALID'}\n",
    "\n",
    "layer2_conv = {'in_size': (27,27), 'dept': 96, 'out_size': (23,23), 'fsize': (5, 5), 'nof':256, 'pad': 'VALID', 'strid': 1}\n",
    "layer2_maxpool = {'in_size': (23,23), 'dept': 256, 'out_size': (11,11), 'fsize': (3, 3), 'strid': 2, 'pad': 'VALID'}\n",
    "\n",
    "layer3_conv = {'in_size': (11,11), 'dept': 256, 'out_size': (9,9), 'fsize': (3, 3), 'nof':384, 'pad': 'VALID', 'strid': 1}\n",
    "\n",
    "layer4_conv = {'in_size': (9,9), 'dept': 384, 'out_size': (7,7), 'fsize': (3, 3), 'nof':384, 'pad': 'VALID', 'strid': 1}\n",
    "\n",
    "layer5_conv = {'in_size': (7,7), 'dept': 384, 'out_size': (5,5), 'fsize': (3, 3), 'nof':256, 'pad': 'VALID', 'strid': 1}\n",
    "layer5_maxpool = {'fsize': (3, 3), 'strid': 2, 'pad': 'VALID'}\n",
    "\n",
    "layer6_fc = {'dept':4*4*256, 'out_size': 4096}\n",
    "\n",
    "layer7_fc = {'dept':4096, 'out_size': 4096}\n",
    "\n",
    "layer8_fc = {'dept':4096, 'out_size': 196}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Layer 1#\n",
    "W_conv1 = weight_variable([layer1_conv['fsize'][0], layer1_conv['fsize'][1], layer1_conv['dept'], layer1_conv['nof']])\n",
    "b_conv1 = bias_variable([layer1_conv['nof']])\n",
    "c_conv1 = tf.nn.conv2d(x, W_conv1, strides=[1, layer1_conv['strid'], layer1_conv['strid'], 1], padding=layer1_conv['pad'])\n",
    "h_conv1 = tf.nn.relu(c_conv1 + b_conv1)\n",
    "Norm1=Normalization(h_conv1)\n",
    "h_pool1 = max_pool_2x2(Norm1)                                        \n",
    "\n",
    "#Layer 2#\n",
    "W_conv2 = weight_variable([layer2_conv['fsize'][0], layer2_conv['fsize'][1], layer2_conv['dept'], layer2_conv['nof']])\n",
    "b_conv2 = bias_variable([layer2_conv['nof']])\n",
    "c_conv2 = tf.nn.conv2d(h_pool1, W_conv2, strides=[1, layer2_conv['strid'], layer2_conv['strid'], 1], padding=layer2_conv['pad'])\n",
    "h_conv2 = tf.nn.relu(c_conv2 + b_conv2)\n",
    "Norm2=Normalization(h_conv2)\n",
    "h_pool2 = max_pool_2x2(Norm2)                                        \n",
    "\n",
    "#Layer 3#\n",
    "W_conv3 = weight_variable([layer3_conv['fsize'][0], layer3_conv['fsize'][1], layer3_conv['dept'], layer3_conv['nof']])\n",
    "b_conv3 = bias_variable([layer3_conv['nof']])\n",
    "c_conv3 = tf.nn.conv2d(h_pool2, W_conv3, strides=[1, layer3_conv['strid'], layer3_conv['strid'], 1], padding=layer3_conv['pad'])\n",
    "h_conv3 = tf.nn.relu(c_conv3 + b_conv3)\n",
    "\n",
    "#Layer 4#\n",
    "W_conv4 = weight_variable([layer4_conv['fsize'][0], layer4_conv['fsize'][1], layer4_conv['dept'], layer4_conv['nof']])\n",
    "b_conv4 = bias_variable([layer4_conv['nof']])\n",
    "c_conv4 = tf.nn.conv2d(h_conv3, W_conv4, strides=[1, layer4_conv['strid'], layer4_conv['strid'], 1], padding=layer4_conv['pad'])\n",
    "h_conv4 = tf.nn.relu(c_conv4 + b_conv4)\n",
    "\n",
    "#Layer 5#\n",
    "W_conv5 = weight_variable([layer5_conv['fsize'][0], layer5_conv['fsize'][1], layer5_conv['dept'], layer5_conv['nof']])\n",
    "b_conv5 = bias_variable([layer5_conv['nof']])\n",
    "c_conv5 = tf.nn.conv2d(h_conv4, W_conv5, strides=[1, layer5_conv['strid'], layer5_conv['strid'], 1], padding=layer5_conv['pad'])\n",
    "h_conv5 = tf.nn.relu(c_conv5 + b_conv5)\n",
    "h_pool5 = max_pool_2x2(h_conv5)                                      \n",
    "\n",
    "#Layer 6 - FC1#\n",
    "in_shape = h_pool5.get_shape()\n",
    "in_size = in_shape.as_list()[1]*in_shape.as_list()[2]*in_shape.as_list()[3]\n",
    "\n",
    "W_fc1 = weight_variable([in_size, layer6_fc['out_size']])\n",
    "b_fc1 = bias_variable([layer6_fc['out_size']])\n",
    "\n",
    "h_pool5_flat = tf.reshape(h_pool5, [-1, in_size])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool5_flat, W_fc1) + b_fc1)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#Layer 7 - FC2#\n",
    "W_fc2 = weight_variable([layer6_fc['out_size'], layer7_fc['out_size']])\n",
    "b_fc2 = bias_variable([layer7_fc['out_size']])\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "#Layer 8 - FC3#\n",
    "W_fc3 = weight_variable([layer7_fc['out_size'], layer8_fc['out_size']])\n",
    "b_fc3 = bias_variable([layer8_fc['out_size']])\n",
    "y_predict = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Optimization Operation\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_predict, y))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_predict,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.02\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "nofsteps = 5000\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "graph_accuracy = np.zeros(nofsteps)\n",
    "\n",
    "for step in range(nofsteps):\n",
    "    offset = (step * batch_size) % (labels.shape[0] - batch_size)\n",
    "    batch_data = data[offset:(offset + batch_size), :, :]\n",
    "    batch_labels = labels[offset:(offset + batch_size)]\n",
    "    graph_accuracy[step] = accuracy.eval(feed_dict={x: batch_data, y: batch_labels, keep_prob: 1.0})\n",
    "    if step%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: batch_data, y: batch_labels, keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(step, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch_data, y: batch_labels, keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
