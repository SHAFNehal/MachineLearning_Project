{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />\n",
    "<img style=\" float:right; display:inline\" src=\"http://opencloud.utsa.edu/wp-content/themes/utsa-oci/images/logo.png\"/>\n",
    "\n",
    "### **University of Texas at San Antonio** \n",
    "<br/>\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 2.5em;\"> **Open Cloud Institute** </span>\n",
    "\n",
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Machine Learning/BigData EE-6973-001-Fall-2016\n",
    "\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Paul Rad, Ph.D.** </span>  \n",
    "\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Ali Miraftab, Research Fellow** </span>\n",
    "  \n",
    "\n",
    "<hr style=\"height:1.5px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:1.5px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 2em;\"> **Car Model Identification Using Convolutional Neural Networks** </span>  \n",
    "<br/>\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.6em;\"> Rajitha Meka, Rupa Nath, Syed Hasib Akhter Faruqui </span> \n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.4em;\"> *Department of Mechanical Engineering* </span>  \n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.4em;\"> *University of Texas at San Antonio, San Antonio, Texas, USA* </span>  \n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.4em;\"> {rajithameka14,nathrupa,shafnehal}@gmail.com </span>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Dataset:** </span> <span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> The image data can be found in [http://vision.stanford.edu/aditya86/ImageNetDogs/ [1],[2]. This directory contains a total of 20,580 annoted images of dogs belonging to 120 different breeds and 150 images per class. Each image is annoted with a bounding box and object class lebel. </span> \n",
    "\n",
    "[1]: http://people.csail.mit.edu/khosla/papers/fgvc2011.pdf\n",
    "[2]: http://www.image-net.org/papers/imagenet_cvpr09.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Outcome:** </span> <span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Our aim is to use a convolutional neural network framework to train and categorize dog breeds </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Project Definition:** </span> <span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> \n",
    "\n",
    "### Start Changing ### Using deep neural network models, ResNet and Convnet, classification of camera images of faces of various people in various poses is sudied. The dataset includes images of 20 different people, approximately 32 images per person, varying the person's expression (happy, sad, angry, neutral), the direction in which they are looking (left, right, straight ahead, up), and whether or not they were wearing sunglassess. </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> There is also variation in the background behind the person, the clothing worn by the person, and theposition of the person's face within the image. I total. 624 greyscale images were collected, each whithin a resolutoin of 120by128, with each image pixel described by a greyscale intensity value between 0 (black) and 255 (white).</span>\n",
    "\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> A variety of target function can be learn from this image data. For example, given an image as input we could train a model to output the identity of the person, the direction in which the person is facing, the gender of the person, whether or not they are wearning sunglasses, etc. All of this target functions can be learned to high accuracy from this image data. In this course research we consider the particular task: learning the direction in which the person is facing (to their left, right, straight ahead, or upward)[1]. </span>\n",
    "\n",
    "[1]: Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao and Li Fei-Fei. Novel dataset for Fine-Grained Image Categorization. First Workshop on Fine-Grained Visual Categorization (FGVC), IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011.\n",
    "\n",
    "[2]: J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li and L. Fei-Fei, ImageNet: A Large-Scale Hierarchical Image Database. IEEE Computer Vision and Pattern Recognition (CVPR), 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"width:830; background-color:white; height:220px; overflow:scroll; overflow-x: scroll;overflow-y: hidden;\">\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"\" width=\"160\" height=\"90\"/>\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"http://vision.stanford.edu/aditya86/ImageNetDogs/images/n02116738-African_hunting_dog/n02116738_288.jpg\" width=\"160\" height=\"90\"/>\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"http://vision.stanford.edu/aditya86/ImageNetDogs/images/n02096294-Australian_terrier/n02096294_1455.jpg\" width=\"160\" height=\"90\"/>\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"http://vision.stanford.edu/aditya86/ImageNetDogs/images/n02093428-American_Staffordshire_terrier/n02093428_19907.jpg\" width=\"160\" height=\"90\"/>\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"http://vision.stanford.edu/aditya86/ImageNetDogs/images/n02096051-Airedale/n02096051_2717.jpg\" width=\"160\" height=\"90\"/>\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"http://vision.stanford.edu/aditya86/ImageNetDogs/images/n02088094-Afghan_hound/n02088094_1234.jpg\" width=\"160\" height=\"90\"/>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
